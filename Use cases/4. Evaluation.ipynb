{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我們建了一個應用，假設是一個問答系統。那我們如何自動衡量這個系統的表現?這樣的衡量在要比較 LLM、Prompt 等等的時候會特別重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "軟體開發中的隕石是比喻指代在隕石落下型開發中，每個步驟都被視為一顆隕石，代表著極高的壓力和快速的進度。這種開發方法強調快速且連貫的開發流程，並在每個步驟中迅速完成所需的任務。因此，隕石在這裡象徵著壓力和迅速完成任務的象徵。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain_setup import ChatOpenAI, tracing_v2_enabled_if_api_key_set\n",
    "\n",
    "loader = NotionDirectoryLoader(\"../data/notion\")\n",
    "splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "documents = splitter.split_documents(loader.load())\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=Qdrant.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        location=\":memory:\",\n",
    "    ).as_retriever(search_kwargs=dict(k=1)), # extract only one document to prevent too long prompt\n",
    "    chain_type_kwargs={\"document_separator\": \"<<<<>>>>>\"},\n",
    ")\n",
    "\n",
    "print(qa_chain.run('什麼是軟體開發中的隕石?'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 自動評量資料生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\121664\\micromamba\\envs\\dev\\lib\\site-packages\\langchain\\chains\\llm.py:349: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qa_pairs': {'query': 'According to the document, what is the cycle of the agile development method?',\n",
       "  'answer': 'The cycle of the agile development method, as described in the document, consists of the following steps: \"要件定義\" (requirement definition), \"基本設計\" (basic design), \"詳細設計\" (detailed design), \"實裝\" (implementation), and \"Programmer\" (programming).'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangSmith URL]: https://smith.langchain.com/o/34ec837d-8405-462d-b949-fdfaebda792b/projects/p/fdcbda35-4d3a-418b-ab49-7e3205e630a6/r/d58b251c-66f6-4bc7-9af9-d5d0bb58166e?poll=true\n"
     ]
    }
   ],
   "source": [
    "example_gen_chain = QAGenerateChain.from_llm(llm=ChatOpenAI())\n",
    "with tracing_v2_enabled_if_api_key_set(project_name='tutorial'):\n",
    "    examples = example_gen_chain.apply_and_parse([{\"doc\": d} for d in documents])\n",
    "    print(len(examples))\n",
    "    display(examples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher coming up with questions to ask on a quiz. \n",
      "Given the following document, please generate a question and answer based on that document.\n",
      "\n",
      "Example Format:\n",
      "<Begin Document>\n",
      "...\n",
      "<End Document>\n",
      "QUESTION: question here\n",
      "ANSWER: answer here\n",
      "\n",
      "These questions should be detailed and be based explicitly on information in the document. Begin!\n",
      "\n",
      "<Begin Document>\n",
      "{doc}\n",
      "<End Document>\n"
     ]
    }
   ],
   "source": [
    "print(example_gen_chain.prompt.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**想想看: 針對產生的問答題，如何確保問題的品質?如何確保答案的正確性?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 自動評價\n",
    "有了資料後，我們可以先產出答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [example['qa_pairs'] for example in examples]\n",
    "predictions = qa_chain.apply(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為產出跟答案都是文字，常常很難用程式判斷意思相不相符，因此我們需要用 LLM 來判斷這件事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangSmith URL]: https://smith.langchain.com/o/34ec837d-8405-462d-b949-fdfaebda792b/projects/p/fdcbda35-4d3a-418b-ab49-7e3205e630a6/r/dd7c343b-c541-4074-b357-95269a11f743?poll=true\n",
      "Example 0:\n",
      "Question: What is the name of the software development methodology introduced in the document?\n",
      "Real Answer: The name of the software development methodology introduced in the document is \"隕石落下型開發\" (Meteorite-style development).\n",
      "Predicted Answer: The name of the software development methodology introduced in the document is \"隕石落下型開發\" (Meteorite-style development).\n",
      "Predicted Grade: CORRECT\n",
      "==================================================\n",
      "Example 1:\n",
      "Question: According to the document, what is the cycle of the agile development method?\n",
      "Real Answer: The cycle of the agile development method, as described in the document, consists of the following steps: \"要件定義\" (requirement definition), \"基本設計\" (basic design), \"詳細設計\" (detailed design), \"實裝\" (implementation), and \"Programmer\" (programming).\n",
      "Predicted Answer: The cycle of the agile development method is not mentioned in the given context.\n",
      "Predicted Grade: CORRECT\n",
      "==================================================\n",
      "Example 2:\n",
      "Question: According to the document, what is one way that feedback can be conveyed to the gods?\n",
      "Real Answer: According to the document, feedback cannot be conveyed to the gods, but prayers can be offered in rare cases.\n",
      "Predicted Answer: According to the document, one way that feedback can be conveyed to the gods is through prayers, although it is mentioned that these prayers can rarely reach their ears.\n",
      "Predicted Grade: CORRECT\n",
      "==================================================\n",
      "Example 3:\n",
      "Question: What are the two events mentioned in the document that can result in the destruction of existing gods?\n",
      "Real Answer: The two events mentioned in the document that can result in the destruction of existing gods are \"諸神黃昏 (Ragnarök)\" and \"聖戰 (Jihad)\".\n",
      "Predicted Answer: The two events mentioned in the document that can result in the destruction of existing gods are \"Ragnarök\" and \"Jihad.\"\n",
      "Predicted Grade: CORRECT\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "eval_chain = QAEvalChain.from_llm(ChatOpenAI())\n",
    "with tracing_v2_enabled_if_api_key_set(project_name='tutorial'):\n",
    "    graded_outputs = eval_chain.evaluate(qa_pairs, predictions)\n",
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['results'])\n",
    "    print(\"==================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: {query}\n",
      "STUDENT ANSWER: {result}\n",
      "TRUE ANSWER: {answer}\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "print(eval_chain.prompt.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**想想看: 評分的 LLM 和被評分的 LLM 甚麼樣的情況下會存在偏心?**\n",
    "\n",
    "<details>\n",
    "<summary>參考</summary>\n",
    "例如同樣是 OpenAI 開發的 GPT 系列，模型架構相似，訓練資料相似，自然產生的結果也會比較相似。評分的模型會挑比較相似的，但這不代表比較正確。\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
